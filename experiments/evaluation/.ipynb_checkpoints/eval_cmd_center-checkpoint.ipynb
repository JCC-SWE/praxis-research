{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca32f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "#2023 questions\n",
    "#2025 questions\n",
    "#upload to blob\n",
    "#download from blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8977a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe7644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from get_qa_texts import pull_qa_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58113238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded qa-2023.txt (86034 characters)\n"
     ]
    }
   ],
   "source": [
    "data = \"qa-2023.txt\"\n",
    "qa_texts = pull_qa_texts(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292ed90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Q: In the paper Embedding-Based Deep Neural Network and Convolutional Neural Network Graph Classifiers, what inspired the authors to use the Doc2Vec model in Natural Language Processing (NLP) for investigating different ways of graph embedding?\n",
      "    A: The authors were inspired by the Doc2Vec model in Natural Language Processing (NLP) for investigating different ways of (sub)graph embedding to represent each graph or subgraph as a fixed-length feature vector.\n",
      "\n",
      "[2] Q: In the paper Analyzing Hong Kong's Legal Judgments from a Computational Linguistics point-of-view, what methods were proposed to analyze legal judgments from Hong Kong's Court System?\n",
      "    A: The methods proposed in the paper include Citation Network Graph Generation, PageRank Algorithm, Keyword Analysis and Summarization, Sentiment Polarity, and Paragraph Classification.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# If blob returned bytes â†’ decode; if string â†’ parse\n",
    "if isinstance(qa_texts, (bytes, bytearray)):\n",
    "    qa_texts = qa_texts.decode(\"utf-8\")\n",
    "if isinstance(qa_texts, str):\n",
    "    qa_texts = json.loads(qa_texts)\n",
    "\n",
    "# Now qa_texts should be a list[dict]\n",
    "for i, qa in enumerate(qa_texts[:2], 1):\n",
    "    print(f\"[{i}] Q: {qa.get('question')}\\n    A: {qa.get('answer')}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fec501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/praxis-research/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start, markers=(\"pyproject.toml\", \".git\", \"README.md\")):\n",
    "    cur = os.path.abspath(start)\n",
    "    while True:\n",
    "        if any(os.path.exists(os.path.join(cur, m)) for m in markers):\n",
    "            return cur\n",
    "        nxt = os.path.dirname(cur)\n",
    "        if nxt == cur:\n",
    "            return None\n",
    "        cur = nxt\n",
    "\n",
    "PROJECT_ROOT = find_project_root('.')\n",
    "if PROJECT_ROOT and PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "current_dir = os.path.abspath('.')\n",
    "parent_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "azure_path = os.path.join(parent_dir, 'azure_resources')\n",
    "blob_path = os.path.join(parent_dir, 'blob_interface')\n",
    "sys.path.insert(0, azure_path)\n",
    "sys.path.insert(0, blob_path)\n",
    "# Import everything at the top\n",
    "from upload_to_blob import upload_to_blob\n",
    "from download_from_blob import download_blob\n",
    "from phase2_dapt_implementation.get_qa_data import get_abstracts_from_blob\n",
    "from get_qa_texts import pull_qa_texts\n",
    "from phase1_model_instantiation.func_test import _build_chat_prompt, generate_reply\n",
    "from phase1_model_instantiation.model_setup import get_qwen_model\n",
    "from extrinsic import evaluate_and_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56a88f-962a-48cc-821c-f30a8395585b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02c9b2d-8a0f-4f07-a5e3-e1837524fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded qa-2023.txt (86034 characters)\n",
      "Downloaded qa-2025.txt (100030 characters)\n"
     ]
    }
   ],
   "source": [
    "data_2023 = pull_qa_texts(data='qa-2023.txt')\n",
    "data_2025 = pull_qa_texts(data='qa-2025.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64793cd-570c-4532-9be6-52874667bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using cached Qwen model at /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n",
      "ðŸ”„ Loading model and tokenizer from /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded Qwen model /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\"\n",
    "model, tokenizer = get_qwen_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563b29ae-32d3-41e9-bc9f-4a26a573f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.79kB [00:00, 267kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'The inspiration for using the Doc2Vec model in the context of this paper likely stems from the need to address the challenges associated with representing and classifying graph data within the realm ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_2023\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqwen_evaluation_2023.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#evaluate_and_save(model, tokenizer, any_other_data, 'custom_evaluation.csv')\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/experiments/evaluation/extrinsic.py:111\u001b[39m, in \u001b[36mevaluate_and_save\u001b[39m\u001b[34m(model, tokenizer, qa_data, output_filename)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_and_save\u001b[39m(model, tokenizer, qa_data, output_filename):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     results = \u001b[43mevaluate_model_on_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     save_results_to_csv(results, output_filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/experiments/evaluation/extrinsic.py:82\u001b[39m, in \u001b[36mevaluate_model_on_qa\u001b[39m\u001b[34m(model, tokenizer, qa_data)\u001b[39m\n\u001b[32m     79\u001b[39m prediction = generate_answer(model, tokenizer, question)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m f1 = \u001b[43mcompute_f1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m rouge = compute_rouge(prediction, ground_truth)\n\u001b[32m     84\u001b[39m bleu = compute_bleu(prediction, ground_truth)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/experiments/evaluation/extrinsic.py:47\u001b[39m, in \u001b[36mcompute_f1_score\u001b[39m\u001b[34m(prediction, ground_truth)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_f1_score\u001b[39m(prediction, ground_truth):\n\u001b[32m     46\u001b[39m     f1_metric = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf1_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/venv/lib/python3.11/site-packages/evaluate/module.py:455\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    452\u001b[39m compute_kwargs = {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs.values()):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize()\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_file_name = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/venv/lib/python3.11/site-packages/evaluate/module.py:520\u001b[39m, in \u001b[36mEvaluationModule.add_batch\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(column) > \u001b[32m0\u001b[39m:\n\u001b[32m    519\u001b[39m             \u001b[38;5;28mself\u001b[39m._enforce_nested_string_type(\u001b[38;5;28mself\u001b[39m.selected_feature_format[key], column[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselected_feature_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer.write_batch(batch)\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (pa.ArrowInvalid, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/venv/lib/python3.11/site-packages/datasets/features/features.py:2079\u001b[39m, in \u001b[36mFeatures.encode_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m   2077\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch.items():\n\u001b[32m   2078\u001b[39m     column = cast_to_python_objects(column)\n\u001b[32m-> \u001b[39m\u001b[32m2079\u001b[39m     encoded_batch[key] = \u001b[43m[\u001b[49m\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/venv/lib/python3.11/site-packages/datasets/features/features.py:2079\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2077\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch.items():\n\u001b[32m   2078\u001b[39m     column = cast_to_python_objects(column)\n\u001b[32m-> \u001b[39m\u001b[32m2079\u001b[39m     encoded_batch[key] = [\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/venv/lib/python3.11/site-packages/datasets/features/features.py:1363\u001b[39m, in \u001b[36mencode_nested_example\u001b[39m\u001b[34m(schema, obj, level)\u001b[39m\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# Object with special encoding:\u001b[39;00m\n\u001b[32m   1361\u001b[39m \u001b[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mencode_example\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1364\u001b[39m \u001b[38;5;66;03m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/venv/lib/python3.11/site-packages/datasets/features/features.py:549\u001b[39m, in \u001b[36mValue.encode_example\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_integer(\u001b[38;5;28mself\u001b[39m.pa_type):\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_floating(\u001b[38;5;28mself\u001b[39m.pa_type):\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(value)\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'The inspiration for using the Doc2Vec model in the context of this paper likely stems from the need to address the challenges associated with representing and classifying graph data within the realm "
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_and_save(model, tokenizer, data_2023, 'qwen_evaluation_2023.csv')\n",
    "#evaluate_and_save(model, tokenizer, any_other_data, 'custom_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea94228-2444-4584-8351-2bf0c68644be",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(model, tokenizer, data_2025, 'qwen_evaluation_2025.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
