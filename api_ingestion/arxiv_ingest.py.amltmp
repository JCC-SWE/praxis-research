import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import time
import sys
import os
import hashlib
from datetime import datetime

# Add cosmos_util path and import CosmosDB
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
cosmos_util_path = os.path.join(parent_dir, 'azure_resources')
sys.path.insert(0, cosmos_util_path)
from cosmos_util import CosmosDB

# Comprehensive AI-related topics
AI_TOPICS = [
    "artificial intelligence",
    "machine learning",
    "deep learning",
    "neural networks",
    "computer vision",
    "natural language processing",
    "reinforcement learning",
    "supervised learning",
    "unsupervised learning",
    "convolutional neural networks",
    "recurrent neural networks",
    "transformer models",
    "generative adversarial networks",
    "large language models",
    "artificial neural networks",
    "pattern recognition",
    "data mining",
    "robotics",
    "expert systems",
    "knowledge representation",
    "automated reasoning",
    "speech recognition",
    "image processing",
    "facial recognition",
    "object detection",
    "semantic segmentation",
    "transfer learning",
    "federated learning",
    "adversarial examples",
    "explainable AI",
    "AI ethics",
    "automated machine learning",
    "multi-agent systems",
    "swarm intelligence",
    "evolutionary algorithms",
    "genetic algorithms",
    "fuzzy logic",
    "Bayesian networks",
    "support vector machines",
    "random forests",
    "ensemble methods",
    "clustering algorithms",
    "dimensionality reduction",
    "feature selection",
    "optimization algorithms",
    "gradient descent",
    "backpropagation",
    "attention mechanisms",
    "BERT",
    "GPT",
    "variational autoencoders",
    "graph neural networks",
    "few-shot learning",
    "zero-shot learning",
    "meta-learning",
    "continual learning",
    "active learning",
    "semi-supervised learning",
    "self-supervised learning",
    "contrastive learning"
]

def build_arxiv_query(topic, start_date=None, end_date=None):
    """
    Build arXiv Lucene-style search query with optional submittedDate range
    """
    topic_encoded = urllib.parse.quote_plus(topic)
    if start_date and end_date:
        start_fmt = start_date.replace("-", "")
        end_fmt = end_date.replace("-", "")
        date_filter = f"submittedDate:[{start_fmt}+TO+{end_fmt}]"
        return f"({date_filter})+AND+all:{topic_encoded}"
    else:
        return f"all:{topic_encoded}"

def title_to_hash(title):
    """Convert title to SHA256 hash for use as ID"""
    return hashlib.sha256(title.encode('utf-8')).hexdigest()

def fetch_arxiv_batch(query_encoded, start, max_results=300, delay=3):
    """
    Fetch a single arXiv API batch using a fully formed encoded query string
    """
    url = f"http://export.arxiv.org/api/query?search_query={query_encoded}&start={start}&max_results={max_results}"
    print(f"üì° Fetching: {url}")
    time.sleep(delay)

    response = urllib.request.urlopen(url)
    xml_data = response.read()
    root = ET.fromstring(xml_data)
    ns = {'atom': 'http://www.w3.org/2005/Atom'}

    records = []
    for entry in root.findall('atom:entry', ns):
        title = entry.find('atom:title', ns).text.strip()
        record = {
            'id': title_to_hash(title),  # Use hash of title as ID
            'arxiv_id': entry.find('atom:id', ns).text,
            'title': title,
            'abstract': entry.find('atom:summary', ns).text.strip(),
            'authors': [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)],
            'published': entry.find('atom:published', ns).text,
            'updated': entry.find('atom:updated', ns).text,
            'primary_category': entry.find('atom:category', ns).attrib.get('term', None),
            'link': next((link.attrib['href'] for link in entry.findall('atom:link', ns)
                          if link.attrib.get('type') == 'text/html'), None),
            'ingested_at': datetime.utcnow().isoformat()
        }
        records.append(record)

    return records

def batch_get_arxiv_to_cosmos_multi_topic(topics=None, papers_per_topic=500, batch_size=100,
                                          container_name="arxiv_container", delay=3,
                                          start_date=None, end_date=None):
    """
    Fetch papers for multiple topics from arXiv and insert into CosmosDB
    """
    if topics is None:
        topics = AI_TOPICS
    
    # Initialize CosmosDB connection
    db = CosmosDB(container_name=container_name)
    
    total_retrieved = 0
    total_inserted = 0
    
    print(f"üöÄ Starting multi-topic arXiv ingest to {container_name}")
    print(f"Topics: {len(topics)} topics")
    print(f"Papers per topic: {papers_per_topic}")
    print(f"Date range: {start_date} to {end_date}")
    print(f"Expected total papers: ~{len(topics) * papers_per_topic}")

    for topic_idx, topic in enumerate(topics, 1):
        print(f"\nüìö Processing topic {topic_idx}/{len(topics)}: '{topic}'")
        
        query_encoded = build_arxiv_query(topic, start_date, end_date)
        topic_retrieved = 0
        topic_inserted = 0
        
        for start in range(0, papers_per_topic, batch_size):
            batch = fetch_arxiv_batch(query_encoded, start=start, max_results=batch_size, delay=delay)
            if not batch:
                print(f"‚ö†Ô∏è No more results for '{topic}'. Moving to next topic.")
                break

            # Insert batch into CosmosDB
            results = db.insert_batch(batch)
            successful_inserts = len([r for r in results if r is not None])
            
            topic_retrieved += len(batch)
            topic_inserted += successful_inserts
            total_retrieved += len(batch)
            total_inserted += successful_inserts
            
            print(f"  üìÑ Batch: {len(batch)} papers | Inserted: {successful_inserts} | Topic total: {topic_retrieved}")

            if len(batch) < batch_size:
                break  # End of results for this topic
        
        print(f"‚úÖ Completed '{topic}': {topic_retrieved} papers retrieved, {topic_inserted} inserted")

    print(f"\nüì¶ FINAL SUMMARY:")
    print(f"üìö Topics processed: {len(topics)}")
    print(f"üìÑ Total papers retrieved: {total_retrieved}")
    print(f"üíæ Total papers inserted: {total_inserted}")

if __name__ == "__main__":
    batch_get_arxiv_to_cosmos_multi_topic(
        topics=AI_TOPICS,  # Use all AI topics, or pass custom list
        papers_per_topic=400,  # Fetch up to 400 papers per topic
        batch_size=100,
        container_name="s_scholar_container",
        start_date="2025-04-30",
        end_date="2025-07-31"
    )