{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c11cd-f98c-4f99-8b52-9acebb66289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phase1_model_instantiation import model_setup, func_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c4efbc-34aa-489a-bb10-1723286d7782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Qwen models:\n",
      "  qwen-2.5-3b: modified 2025-09-04 22:55:50+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Available Qwen models:\")\n",
    "models = model_setup.list_qwen_models()\n",
    "for name, info in models.items():\n",
    "    print(f\"  {name}: modified {info['last_modified']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509ec020-d5d1-429f-96ed-5dd7085f4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in model directory:\n",
      "  cache\n",
      "\n",
      "‚ùå config.json is missing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_path = \"/workspace/praxis-research/base-model/qwen-2.5-3b\"\n",
    "print(\"Files in model directory:\")\n",
    "for file in os.listdir(model_path):\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "# Check if config.json exists and what's in it\n",
    "config_path = os.path.join(model_path, \"config.json\")\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        import json\n",
    "        config = json.load(f)\n",
    "        print(f\"\\nConfig model_type: {config.get('model_type', 'MISSING')}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå config.json is missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c6f9db-e4ee-413f-96f7-c600053f4bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All blobs in qwen_models/:\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/07bfe0640cb5a0037f9322287fbfc682806cf672.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/38047c6284a25427043f0ab040f623a2a20dd093.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/443909a61d429dff23010e5bddd28ff530edda00.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/4783fe10ac3adce15ac8f358ef5462739852c569.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/51b83fbd4e2d363b60235b9c8d494046a83c1cbf.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/67347b23fb4165b652eb6611f5e1f2a06dfcddba8e909df1b2b0b1857bee06c2.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/a40d941d0e7e0b966ad8b62bb6d6b7c88cce1299197b599d9d0a4ce59aabfc1d.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/.locks/models--Qwen--Qwen2.5-3B-Instruct/bf077f03dc569cfb8a90b3ec1ad20365a620bad6.lock (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/.no_exist/aa8e72537993ba99e69dfaafa59ed015b17504d1/added_tokens.json (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/.no_exist/aa8e72537993ba99e69dfaafa59ed015b17504d1/chat_template.jinja (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/.no_exist/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/.no_exist/aa8e72537993ba99e69dfaafa59ed015b17504d1/special_tokens_map.json (0 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/07bfe0640cb5a0037f9322287fbfc682806cf672 (7305 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0 (1671839 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/38047c6284a25427043f0ab040f623a2a20dd093 (35581 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/443909a61d429dff23010e5bddd28ff530edda00 (7031645 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/4783fe10ac3adce15ac8f358ef5462739852c569 (2776833 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/51b83fbd4e2d363b60235b9c8d494046a83c1cbf (661 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/67347b23fb4165b652eb6611f5e1f2a06dfcddba8e909df1b2b0b1857bee06c2 (3968658944 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/a40d941d0e7e0b966ad8b62bb6d6b7c88cce1299197b599d9d0a4ce59aabfc1d (2203268048 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/blobs/bf077f03dc569cfb8a90b3ec1ad20365a620bad6 (242 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/refs/main (40 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json (661 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json (242 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt (1671839 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model-00001-of-00002.safetensors (3968658944 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model-00002-of-00002.safetensors (2203268048 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json (35581 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json (7031645 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json (7305 bytes)\n",
      "  qwen_models/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json (2776833 bytes)\n",
      "  qwen_models/qwen-2.5-3b/manifest.json (4354 bytes)\n"
     ]
    }
   ],
   "source": [
    "# List all blobs in the qwen_models directory\n",
    "\n",
    "secrets = model_setup.get_secrets()\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={secrets['storage_account_name']};AccountKey={secrets['storage_account_key']};EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(secrets['container'])\n",
    "\n",
    "print(\"All blobs in qwen_models/:\")\n",
    "blobs = list(container_client.list_blobs(name_starts_with=\"qwen_models/\"))\n",
    "for blob in blobs:\n",
    "    print(f\"  {blob.name} ({blob.size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215df5e0-3026-4241-8843-fac1aebeebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using cached Qwen model at /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n",
      "üîÑ Loading model and tokenizer from /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf3eeb7b3084bc3a422261f725f1daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded Qwen model /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n"
     ]
    }
   ],
   "source": [
    "# Load model/tokenizer from the correct cache path\n",
    "model_path = \"/workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\"\n",
    "\n",
    "model, tokenizer = model_setup.get_qwen_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1565e3-80ae-4b2e-95f0-229683675b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactive QA/chat with Qwen. Type 'exit' or 'quit' to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  are you there?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen: Yes, I am here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start interactive chat\n",
    "func_test.chat_loop(model, tokenizer, system_text=\"You are a helpful AI assistant.\", max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f01740-1e11-4590-a64d-00ebd9e8315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import data_pipeline, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ca368f-b4e4-4dd1-b930-9bf169f93556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting credentials...\n",
      "Creating Key Vault client...\n",
      "Retrieving secrets...\n",
      "üìä Retrieving all data from s_scholar_container...\n",
      "Read 135852 items\n",
      "üìÑ Processing 135852 records for NLP...\n",
      "‚úÖ Formatted 135852 records for NLP processing\n",
      "üìÑ Found 135852 records from Semantic Scholar\n",
      "üîÑ Preprocessing 135852 records for Qwen/Qwen2.5-3B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bb27efda854bd482617f32141900cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765314f3032f48aa919ff3a4d5963ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37db3a425fbe433c83c16a8ba00f6508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6693741cd8a748fdbbb2c608d43f685b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned data: 135491 records\n",
      "üîÑ Tokenizing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d7039e92284959ad8992d14b0ef237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized 135491 samples\n",
      "üíæ Saving processed data to qwen_processed_data.pkl\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "Total samples: 135491\n",
      "Average tokens per sample: 257.1\n",
      "Max tokens: 1024\n",
      "Min tokens: 6\n",
      "\n",
      "Source distribution:\n",
      "  s_scholar_container: 135491 samples\n",
      "\n",
      "‚úÖ Preprocessing complete! Dataset ready for training.\n"
     ]
    }
   ],
   "source": [
    "data = preprocessing.get_nlp_ready_data('s_scholar_container')\n",
    "    \n",
    "if data:\n",
    "    print(f\"üìÑ Found {len(data)} records from Semantic Scholar\")\n",
    "    \n",
    "    # Preprocess for Qwen\n",
    "    dataset = preprocessing.preprocess_for_qwen(\n",
    "        data, \n",
    "        model_name=\"Qwen/Qwen2.5-3B\",\n",
    "        max_length=1024,  # Adjust based on your needs\n",
    "        save_path=\"data/qwen_processed_data.pkl\"\n",
    "    )\n",
    "    \n",
    "    # Show statistics\n",
    "    preprocessing.get_dataset_stats(dataset)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete! Dataset ready for training.\")\n",
    "else:\n",
    "    print(\"‚ùå No data found in s_scholar_container!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69ef753-7594-437b-be44-22e533119542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phase2_dapt_implementation.openai_client import GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb850c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081bc6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = gpt.ask(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c76ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Recurrent Neural Network (RNN) is a type of neural network designed to handle sequential data, such as time series data or natural language. Unlike feedforward neural networks, which have a fixed input size, RNNs have connections that create loops in the network, allowing information to persist. This enables RNNs to learn from and make predictions about sequences of data. RNNs are commonly used in tasks such as language modeling, speech recognition, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "print(gpt.ask(\"What is an RNN in machine learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bb4cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-keyvault-secrets\n",
      "  Obtaining dependency information for azure-keyvault-secrets from https://files.pythonhosted.org/packages/26/94/7c902e966b28e7cb5080a8e0dd6bffc22ba44bc907f09c4c633d2b7c4f6a/azure_keyvault_secrets-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading azure_keyvault_secrets-4.10.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: azure-identity in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (1.17.1)\n",
      "Collecting azure-cosmos\n",
      "  Obtaining dependency information for azure-cosmos from https://files.pythonhosted.org/packages/61/dc/380f843744535497acd0b85aacb59565c84fc28bf938c8d6e897a858cd95/azure_cosmos-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading azure_cosmos-4.9.0-py3-none-any.whl.metadata (80 kB)\n",
      "     ---------------------------------------- 0.0/80.8 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/80.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 80.8/80.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-keyvault-secrets) (0.6.1)\n",
      "Collecting azure-core>=1.31.0 (from azure-keyvault-secrets)\n",
      "  Obtaining dependency information for azure-core>=1.31.0 from https://files.pythonhosted.org/packages/27/52/805980aa1ba18282077c484dba634ef0ede1e84eec8be9c92b2e162d0ed6/azure_core-1.35.1-py3-none-any.whl.metadata\n",
      "  Downloading azure_core-1.35.1-py3-none-any.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.5/46.5 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-keyvault-secrets) (4.12.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-identity) (41.0.3)\n",
      "Requirement already satisfied: msal>=1.24.0 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-identity) (1.30.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-identity) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-core>=1.31.0->azure-keyvault-secrets) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from azure-core>=1.31.0->azure-keyvault-secrets) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from cryptography>=2.5->azure-identity) (1.15.1)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from msal>=1.24.0->azure-identity) (2.4.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from msal-extensions>=0.3.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=0.3.0->azure-identity) (305.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-keyvault-secrets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-keyvault-secrets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-keyvault-secrets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshuacubero\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-keyvault-secrets) (2024.2.2)\n",
      "Using cached azure_keyvault_secrets-4.10.0-py3-none-any.whl (125 kB)\n",
      "Using cached azure_cosmos-4.9.0-py3-none-any.whl (303 kB)\n",
      "Downloading azure_core-1.35.1-py3-none-any.whl (211 kB)\n",
      "   ---------------------------------------- 0.0/211.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 211.8/211.8 kB 12.6 MB/s eta 0:00:00\n",
      "Installing collected packages: azure-core, azure-keyvault-secrets, azure-cosmos\n",
      "  Attempting uninstall: azure-core\n",
      "    Found existing installation: azure-core 1.30.2\n",
      "    Uninstalling azure-core-1.30.2:\n",
      "      Successfully uninstalled azure-core-1.30.2\n",
      "Successfully installed azure-core-1.35.1 azure-cosmos-4.9.0 azure-keyvault-secrets-4.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-keyvault-secrets azure-identity azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a78f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phase2_dapt_implementation.get_qa_data import get_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403f8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_papers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97b8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phase2_dapt_implementation.get_qa_data import process_papers_to_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c6286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_papers_to_blob(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e3921c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphase2_dapt_implementation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_qa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_qa_pipeline\n",
      "File \u001b[1;32mc:\\Users\\joshuacubero\\Documents\\praxis-research\\phase2_dapt_implementation\\create_qa.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, blob_path)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mblob_interface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mupload_to_blob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upload_to_blob\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT \n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mget_qa_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_abstracts_from_blob\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_abstracts_text\u001b[39m(abstracts_text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai_client'"
     ]
    }
   ],
   "source": [
    "from phase2_dapt_implementation.create_qa import create_qa_pipeline\n",
    "#create_qa_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3585e489-b5fc-412b-866b-2e7d26f52bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./phase2_dapt_implementation')\n",
    "\n",
    "from phase2_dapt_implementation.domain_adaptation import DAPTTrainer, load_processed_data\n",
    "from phase1_model_instantiation.model_setup import get_qwen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8b4ea7-82b8-4cb5-b0cc-c2f59b6e1423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using cached Qwen model at /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n",
      "üîÑ Loading model and tokenizer from /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821603ce5b674cf3b5acc428eb06b62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded Qwen model /workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\n",
      "Loaded dataset: 135,491 examples\n",
      "Tokenizer: Qwen/Qwen2.5-3B\n",
      "Max length: 1024\n",
      "Vocab size: 151,643\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/workspace/praxis-research/base-model/qwen-2.5-3b/cache/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1\"\n",
    "model, tokenizer = get_qwen_model(model_path)\n",
    "train_dataset = load_processed_data(\"./data/qwen_processed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd36c42-7095-4694-a1a6-30aa1cadb7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/qwen_processed_data.pkl\n",
      "Size: 963,167,065 bytes\n",
      "Size: 940592.84 KB\n",
      "Size: 918.55 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_file_size(filename):\n",
    "    \"\"\"Check the size of a file and display it in different units.\"\"\"\n",
    "    try:\n",
    "        # Get file size in bytes\n",
    "        file_size_bytes = os.path.getsize(filename)\n",
    "        \n",
    "        # Convert to different units\n",
    "        file_size_kb = file_size_bytes / 1024\n",
    "        file_size_mb = file_size_kb / 1024\n",
    "        file_size_gb = file_size_mb / 1024\n",
    "        \n",
    "        print(f\"File: {filename}\")\n",
    "        print(f\"Size: {file_size_bytes:,} bytes\")\n",
    "        print(f\"Size: {file_size_kb:.2f} KB\")\n",
    "        print(f\"Size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        if file_size_gb >= 1:\n",
    "            print(f\"Size: {file_size_gb:.2f} GB\")\n",
    "            \n",
    "        return file_size_bytes\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check the size of your pickle file\n",
    "filename = \"data/qwen_processed_data.pkl\"\n",
    "file_size = check_file_size(filename)\n",
    "\n",
    "# Alternative: Just get the size in bytes\n",
    "# file_size_bytes = os.path.getsize(\"qwen_processed_data.pkl\")\n",
    "# print(f\"File size: {file_size_bytes} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6d6f1c-ce20-477f-8814-51c035def55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset loaded: {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02833659-4beb-4b39-a7d7-9b70ed3b8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"data/qwen_processed_data.pkl\", 'rb') as f:\n",
    "#     dataset = pickle.load(f)\n",
    "\n",
    "# print(f\"Dataset type: {type(dataset)}\")\n",
    "# print(f\"Dataset length: {len(dataset)}\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# # Check what methods/attributes are available\n",
    "# print(\"Available methods/attributes:\")\n",
    "# methods = [method for method in dir(dataset) if not method.startswith('_')]\n",
    "# print(methods[:10])  # Show first 10 methods\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# # Check if it's a HuggingFace Dataset\n",
    "# if hasattr(dataset, 'features'):\n",
    "#     print(\"This appears to be a HuggingFace Dataset\")\n",
    "#     print(f\"Features: {dataset.features}\")\n",
    "#     print(f\"Column names: {dataset.column_names}\")\n",
    "#     print(f\"Shape: {dataset.shape}\")\n",
    "    \n",
    "#     # Access first example properly\n",
    "#     first_example = dataset[0]\n",
    "#     print(f\"\\nFirst example keys: {first_example.keys()}\")\n",
    "    \n",
    "#     for key, value in first_example.items():\n",
    "#         print(f\"{key}: {type(value)}\")\n",
    "#         if isinstance(value, list):\n",
    "#             print(f\"  Length: {len(value)}\")\n",
    "#         elif isinstance(value, str):\n",
    "#             print(f\"  String length: {len(value)} chars\")\n",
    "\n",
    "# # Check if it's a dictionary\n",
    "# elif isinstance(dataset, dict):\n",
    "#     print(\"This is a dictionary\")\n",
    "#     print(f\"Keys: {list(dataset.keys())}\")\n",
    "    \n",
    "#     for key, value in dataset.items():\n",
    "#         print(f\"{key}: {type(value)}, length: {len(value) if hasattr(value, '__len__') else 'N/A'}\")\n",
    "\n",
    "# # Check if it's some other iterable\n",
    "# else:\n",
    "#     print(\"Trying to iterate through dataset:\")\n",
    "#     try:\n",
    "#         for i, item in enumerate(dataset):\n",
    "#             if i >= 3:  # Only show first 3 items\n",
    "#                 break\n",
    "#             print(f\"Item {i}: {type(item)}\")\n",
    "#             if hasattr(item, 'keys'):\n",
    "#                 print(f\"  Keys: {list(item.keys())}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Cannot iterate: {e}\")\n",
    "        \n",
    "#     # Try alternative access methods\n",
    "#     try:\n",
    "#         print(\"\\nTrying .select() method (HuggingFace Dataset):\")\n",
    "#         sample = dataset.select([0])\n",
    "#         print(f\"Sample: {type(sample)}\")\n",
    "#     except:\n",
    "#         print(\"No .select() method available\")\n",
    "        \n",
    "#     try:\n",
    "#         print(\"\\nTrying list conversion:\")\n",
    "#         dataset_list = list(dataset)\n",
    "#         print(f\"Converted to list, length: {len(dataset_list)}\")\n",
    "#         print(f\"First item type: {type(dataset_list[0])}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Cannot convert to list: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2c302-b1f1-41dc-978f-996fff81019c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ccee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up DataParallel for 5 GPUs\n",
      "Training will use 5 GPU(s)\n",
      "Batch configuration:\n",
      "  Per-GPU batch size: 12\n",
      "  Effective batch size: 60\n",
      "  Gradient accumulation steps: 2\n",
      "  Actual total batch size: 120\n",
      "Training setup complete for 3 epochs\n",
      "Using DataParallel training\n",
      "Starting DAPT training...\n",
      "Starting DAPT training...\n",
      "Dataset size: 2 examples\n",
      "Training interrupted: Caught AttributeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 46, in __call__\n",
      "    return self.torch_call(features)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 1018, in torch_call\n",
      "    \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 437, in _torch_collate_batch\n",
      "    length_of_first = examples[0].size(0)\n",
      "                      ^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Dataset' object has no attribute 'size'\n",
      "\n",
      "Progress saved at step 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 46, in __call__\n    return self.torch_call(features)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 1018, in torch_call\n    \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 437, in _torch_collate_batch\n    length_of_first = examples[0].size(0)\n                      ^^^^^^^^^^^^^^^^\nAttributeError: 'Dataset' object has no attribute 'size'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Execute training\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting DAPT training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m training_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Get metrics\u001b[39;00m\n\u001b[32m     25\u001b[39m training_metrics = trainer.get_training_metrics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/phase2_dapt_implementation/domain_adaptation.py:199\u001b[39m, in \u001b[36mDAPTTrainer.train\u001b[39m\u001b[34m(self, train_dataset, resume_from_checkpoint)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProgress saved at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.training_results = {\n\u001b[32m    189\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtraining_time_minutes\u001b[39m\u001b[33m\"\u001b[39m: training_time / \u001b[32m60\u001b[39m,\n\u001b[32m    190\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfinal_loss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    196\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnum_gpus_used\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.effective_gpus\n\u001b[32m    197\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/praxis-research/phase2_dapt_implementation/domain_adaptation.py:161\u001b[39m, in \u001b[36mDAPTTrainer.train\u001b[39m\u001b[34m(self, train_dataset, resume_from_checkpoint)\u001b[39m\n\u001b[32m    157\u001b[39m start_time = time.time()\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     train_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     training_time = time.time() - start_time\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mAttributeError\u001b[39m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 46, in __call__\n    return self.torch_call(features)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 1018, in torch_call\n    \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 437, in _torch_collate_batch\n    length_of_first = examples[0].size(0)\n                      ^^^^^^^^^^^^^^^^\nAttributeError: 'Dataset' object has no attribute 'size'\n"
     ]
    }
   ],
   "source": [
    "# Updated Jupyter command-center.ipynb code\n",
    "\n",
    "# Initialize trainer for 5 GPUs (UPDATED: use_multi_gpu instead of num_gpus)\n",
    "trainer = DAPTTrainer(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    output_dir=\"./phase2_dapt_implementation/ai_dapt\", \n",
    "    use_multi_gpu=True  # This automatically uses all available GPUs with DataParallel\n",
    ")\n",
    "\n",
    "# Setup training configuration\n",
    "trainer.setup_training(\n",
    "    num_epochs=3,\n",
    "    batch_size_per_gpu=12,  # Optimized for H200s\n",
    "    target_batch_size=120,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "# Execute training\n",
    "print(\"Starting DAPT training...\")\n",
    "training_results = trainer.train(train_dataset)\n",
    "\n",
    "# Get metrics\n",
    "training_metrics = trainer.get_training_metrics()\n",
    "\n",
    "# Enhanced results display\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"DAPT TRAINING COMPLETED\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Time: {training_results['training_time_minutes']:.1f} minutes ({training_results['training_time_minutes']/60:.1f} hours)\")\n",
    "print(f\"Final Loss: {training_results.get('final_loss', 'N/A')}\")\n",
    "print(f\"Total Steps: {training_results.get('total_steps', 'N/A'):,}\")\n",
    "print(f\"GPUs Used: {training_results.get('num_gpus_used', 'N/A')}\")\n",
    "print(f\"Dataset Size: {training_results.get('dataset_size', 'N/A'):,} examples\")\n",
    "print(f\"Model saved: {training_results.get('model_path', 'N/A')}\")\n",
    "print(f\"Success: {training_results.get('completed_successfully', False)}\")\n",
    "\n",
    "# Display detailed metrics\n",
    "if training_metrics:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    feasibility = training_metrics.get('training_feasibility', {})\n",
    "    efficiency = training_metrics.get('computational_efficiency', {})\n",
    "    \n",
    "    print(f\"Convergence: {'‚úÖ Converged' if feasibility.get('converged', False) else '‚ö†Ô∏è Check convergence'}\")\n",
    "    print(f\"Processing Rate: {feasibility.get('examples_per_minute', 0):.1f} examples/minute\")\n",
    "    print(f\"Model Parameters: {efficiency.get('trainable_params', 'N/A')} trainable\")\n",
    "    print(f\"Training Mode: {efficiency.get('training_mode', 'N/A')}\")\n",
    "    print(f\"GPU Memory Used: {efficiency.get('gpu_memory_used_gb', 'N/A')} GB per GPU\")\n",
    "    print(f\"GPU Utilization: {efficiency.get('gpu_utilization', 'N/A')}\")\n",
    "\n",
    "# Optional: Save results to file\n",
    "import json\n",
    "results_summary = {\n",
    "    \"training_results\": training_results,\n",
    "    \"training_metrics\": training_metrics,\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "with open(f\"./phase2_dapt_implementation/training_summary_{time.strftime('%Y%m%d_%H%M%S')}.json\", 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìÑ Results saved to training summary file\")\n",
    "print(f\"üéâ DAPT training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BLOCK 2: DAPT ENGINE EVALUATION =====\n",
    "import sys\n",
    "sys.path.append('./phase2_dapt_implementation')\n",
    "\n",
    "from phase2_dapt_implementation.dapt_engine import ModelInteractor, DAPTEvaluator, load_dapt_model\n",
    "\n",
    "# Load the trained DAPT model\n",
    "model_path = \"./phase2_dapt_implementation/ai_dapt_final\"  # From training block\n",
    "dapt_model, dapt_tokenizer = load_dapt_model(model_path)\n",
    "\n",
    "# Initialize interactor\n",
    "interactor = ModelInteractor(dapt_model, dapt_tokenizer)\n",
    "\n",
    "# Test domain adaptation\n",
    "print(\"=== Domain Adaptation Testing ===\")\n",
    "domain_results = interactor.test_domain_adaptation()\n",
    "\n",
    "# Custom AI questions test\n",
    "ai_questions = [\n",
    "    \"What is zero-shot domain adaptation?\",\n",
    "    \"How does 3D software generation differ from 2D interface creation?\",\n",
    "    \"What makes human-AI collaboration challenging in physical environments?\",\n",
    "    \"Why do synthetic datasets from text-to-image models have alignment problems?\",\n",
    "    \"What is the main limitation of existing SMT model counting approaches?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Custom AI Questions Test ===\")\n",
    "custom_results = interactor.test_domain_adaptation(ai_questions)\n",
    "\n",
    "# Full evaluation with mock training results\n",
    "mock_training_results = {\n",
    "    \"dataset_size\": 100000,\n",
    "    \"training_time_minutes\": 68,\n",
    "    \"final_loss\": 2.1\n",
    "}\n",
    "\n",
    "print(\"\\n=== Comprehensive Evaluation ===\")\n",
    "evaluator = DAPTEvaluator(dapt_model, dapt_tokenizer, mock_training_results)\n",
    "efficiency_metrics = evaluator.calculate_model_efficiency()\n",
    "resource_analysis = evaluator.analyze_resource_sufficiency()\n",
    "final_report = evaluator.generate_feasibility_report(domain_results)\n",
    "\n",
    "print(f\"\\nFinal DAPT Assessment: {final_report['verdict']}\")\n",
    "print(f\"Domain Adaptation Score: {domain_results['avg_domain_score']:.2f}\")\n",
    "\n",
    "# Uncomment for interactive chat\n",
    "# print(\"\\n=== Interactive Chat (uncomment to use) ===\")\n",
    "# interactor.interactive_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
